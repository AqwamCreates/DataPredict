# Release Version 2.0

## All

* All internal components now uses TensorL2D instead of MatrixL for full compatibility with DataPredict Neural. TensorL2D can be replaced with TensorL.

* All constructors now requires a parameter dictionary instead of arguments.

## Models

* Added SoftActorCritic, DeepDeterministicPolicyGradient and TwinDelayedDeepDeterministicPolicyGradient

* DeepQLearning, DeepStateActionRewardStateAction, DeepExpectedStateActionRewardStateAction models and its variants now have "lambda" argument for TD-Lambda functionality. This includes AdvantageActorCritic model.

* The diagonalGaussianUpdate() function now requires actionNoiseVector.

* All reinforcement learning models now require "terminalStateValue" for categoricalUpdate(), diagonalGaussianUpdate() and episodeUpdate() functions.

* Reimplemented ActorCritic, VanillaPolicyGradient and REINFORCE models.

* Removed AsynchronousAdvantageActorCritic.

## AqwamCustomModels

* Removed "AqwamCustomModels" section and its models.

# Others

* Moved RandomNetworkDistillation, GenerativeAdversarialImitationLearning and WassersteinGenerativeAdversarialImitationLearning to "ReinforcementLearningStrategies" section.

* Renamed DistributedGradients to DistributedGradientsCoordinator and moved to "DistributedTrainingStrategies" section.

* Renamed DistributedModelParameters to DistributedModelParametersCoordinator and moved to "DistributedTrainingStrategies" section.

## QuickSetups

* Fixed CategoricalPolicyQuickSetup and DiagonalGaussianPolicyQuickSetup, where the next action is used in the categoricalUpdate() and diagonalGaussianUpdate() instead of previous action.